#!pip install spacy
#!python -m spacy download en_core_web_sm
import kagglehub
adilshamim8_mrbeast_youtube_comment_sentiment_analysis_path = kagglehub.dataset_download('adilshamim8/mrbeast-youtube-comment-sentiment-analysis')

print('Data source import complete.')
import os
import pandas as pd

Datatra = pd.read_csv(os.path.join(adilshamim8_mrbeast_youtube_comment_sentiment_analysis_path, 'sentiment_analysis_dataset.csv'), on_bad_lines='skip')

Datatra.head()
Datatra.info()
# Removing nulls
Datatra.dropna(subset=['Comment'], inplace=True)

print(f'Train nulls:\n{Datatra.isna().sum()}')
Datatra['Sentiment'].value_counts()
Datatra['Sentiment'] = Datatra['Sentiment'].str.strip().str.capitalize()
import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='Sentiment', data=Datatra, palette='viridis')
plt.show()
Datatra['Sentiment'].value_counts()
from wordcloud import WordCloud
plt.figure(figsize=(8, 6))
wc = WordCloud(max_words=100, width=1600, height=800).generate(" ".join(Datatra['Comment']))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')  # Hide axes for better visualization
plt.show()
import spacy as spc
tokenizer = spc.load('en_core_web_sm')

def preprocessing_pipe(texts):
    docs = tokenizer.pipe(
        texts,
        n_process=4,  # No of CPUs
        batch_size=64,
    )
    for doc in docs:
        yield " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])
from tqdm import tqdm
# Applying preprocessing to each tweet in the Series
tqdm.pandas()
Datatra['Cleaned Comment'] = list(preprocessing_pipe(Datatra['Comment']))
from sklearn.feature_extraction.text import TfidfVectorizer
def vectorization(tratxt, valtxt=None):
    vect = TfidfVectorizer()
    x_train = vect.fit_transform(tratxt)
    if valtxt is not None:
        x_val = vect.transform(valtxt)
        return x_train, x_val, vect
    return x_train, vect
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Datatra['Sentiment'] = le.fit_transform(Datatra['Sentiment'])
print(Datatra.shape)

from tqdm import tqdm
from sklearn.model_selection import train_test_split

x, vect = vectorization(Datatra['Cleaned Comment'])
y = Datatra['Sentiment']

x_dense = x.toarray()
x_train, x_test, y_train, y_test = train_test_split(x_dense, y, test_size=0.2, random_state=42, stratify=y)
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

model = Sequential(
    [
        Dense(64, activation='relu', input_dim=x_train.shape[1]),
        Dense(32, activation='relu'),
        Dense(y_train.nunique(), activation='softmax')
    ]
)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# Handling Data Imbalances
from sklearn.utils import class_weight
import numpy as np

y_full_encoded = Datatra['Sentiment']

weights_array = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_full_encoded),
    y=y_full_encoded
)

class_weights_dict = dict(enumerate(weights_array))

print(f"Used Weights: {class_weights_dict}")
model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), class_weight=class_weights_dict)
model.evaluate(x_test, y_test)
model.save('model.h5')
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

y_preds = model.predict(x_test)
y_pred_classes = np.argmax(y_preds, axis=1) # Convert probabilities to class labels
print(classification_report(y_test, y_pred_classes))
